{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a6946d4-c620-46fa-829e-4cb7c0bf56ec",
   "metadata": {},
   "source": [
    "# Spacy/Medspacy IAA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf1275d-aea3-42ff-bb49-a70af27e8724",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4767b5a-d4ea-472a-b95b-224a5f3ac8fd",
   "metadata": {},
   "source": [
    "Prodigy forum answer about IAA for spans https://support.prodi.gy/t/proper-way-to-calculate-inter-annotator-agreement-for-spans-ner/5760\n",
    "\n",
    "Spacy scorer object https://spacy.io/api/scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae97f4-619f-4f09-8e87-767767b5830b",
   "metadata": {},
   "source": [
    "## End Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759cea6f-2684-4b0b-bb00-f8541df72e6b",
   "metadata": {},
   "source": [
    "### Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa04fb-5095-4672-b315-43791d53fcca",
   "metadata": {},
   "source": [
    "Provide a collection of methods to evaluate IAA between _n_ arbitrary spacy `doc` objects. Provide methods that aid in error analysis such as providing lists of differences.\n",
    "\n",
    "Priorities:\n",
    "* Pairwise F1\n",
    "    * configurable strict/loose matching\n",
    "    * configurable inclusion of labels/attributes (calculate just span vs span+class agreement)\n",
    "\n",
    "* Imported python files\n",
    "    * reasonable docstrings on methods/classes\n",
    "    \n",
    "* Unit tests\n",
    "    * add CI to repo for automated testing later\n",
    "\n",
    "Extra features:\n",
    "* List of differences between docs\n",
    "* \n",
    "\n",
    "Expected challenges\n",
    "* Spacy scorer functions are useful, but _only_ do strict span matching\n",
    "* Fewer resources (obviously?) available for comparisons between 3+ docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb0fb6",
   "metadata": {},
   "source": [
    "# Testing Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07738af3-f6f3-4868-be7b-69258b0ec3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ce3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eea5c733-d512-4bed-b643-cb0a5a01915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp2 = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919e2eea-0032-4232-95a8-a57085cb3310",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"this is a test document made in utah or mississippi, or salt lake city.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add063d2-cb64-451f-8dda-65cc9cd719e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(utah, mississippi)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bda30c7e-df45-4e2e-b63e-916da8f1ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp2(\"this is a test document made in utah or mississippi, or salt lake city.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20aaf660-1860-489a-a326-06347f7b78d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(utah, mississippi, lake city)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ecb69f-9a0e-4930-ab72-36dea261fea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed28ee93-5f7a-4a34-b610-44467c63c92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a test', 'PERSON'), ('utah', 'GPE'), ('mississippi', 'PERSON'), ('lake', 'GPE'), ('city', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "spand = list()\n",
    "spand += [Span(doc, 2, 4, label=\"PERSON\"),Span(doc,7,8,label=\"GPE\"),Span(doc,9,10,label=\"PERSON\"),Span(doc,13,14,label=\"GPE\"),Span(doc,14,15,label=\"GPE\")]\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = spand\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13836407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 2\n",
      "0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "#Run below cells before this\n",
    "tp,fp,fn = agreement(doc,doc2,1,1)\n",
    "print(tp,fp,fn)\n",
    "print(pairwise_f1(tp,fp,fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098164e2",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f8afd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quicksectx import IntervalNode, IntervalTree, Interval #note that you need the quicksectx library\n",
    "\n",
    "#In order to make the code a little more adaptable for situations of multiple overlapping entities, as well as for \n",
    "#transparency and testing the code, I wrote the overlaps code to output a mapping of which entities are being matched. \n",
    "#Then agreement can parse this output for how many valid overlaps exist.\n",
    "\n",
    "#This makes the code a little more complicated to understand, but I think it makes everything more transparent and adaptable.\n",
    "\n",
    "def overlaps(doc1_ents, doc2_ents,labels):\n",
    "    '''Calculates overlapping entities between two spacy documents. Also checks for matching labels if label=1.\n",
    "    \n",
    "    Return:\n",
    "        Dictionaries with the mapping of matching entity indices:\n",
    "            keys: entity index from one annotation\n",
    "            value: matched entity index from other annotation\n",
    "        \n",
    "        Ex: \"{1 : [2] , 3 : [4,5]}\" means that entity 1 from doc1 matches entity 1 in doc2, and entity 3 in doc1 matches \n",
    "        entity 4 and 5 from doc2.\n",
    "    '''\n",
    "    \n",
    "    doc1_matches = dict()\n",
    "    doc2_matches = dict()\n",
    "    \n",
    "    tree = IntervalTree()\n",
    "    for index2,ent2 in enumerate(doc2_ents):\n",
    "        tree.add(ent2.start_char,ent2.end_char,index2)\n",
    "    \n",
    "    for index1,ent1 in enumerate(doc1_ents):\n",
    "        matches = tree.search(ent1.start_char,ent1.end_char)\n",
    "        for match in matches:\n",
    "            index2 = match.data #match.data is the index of doc2_ents\n",
    "            if ((labels == 0) | (doc2_ents[index2].label_ == ent1.label_)):\n",
    "                if index1 not in doc1_matches.keys():\n",
    "                    doc1_matches[index1] = [index2]\n",
    "                else:\n",
    "                    doc1_matches[index1].append(index2)\n",
    "                if index2 not in doc2_matches.keys():\n",
    "                    doc2_matches[index2] = [index1]\n",
    "                else:\n",
    "                    doc2_matches[index2].append(index1)\n",
    "                \n",
    "    return doc1_matches, doc2_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9b138a1-a375-4503-93fd-76cf9e2721b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the old, less efficient code. The newer code uses a tree search instead of the nested for-loop.\n",
    "\n",
    "def old_overlaps(doc1_ents, doc2_ents,labels):\n",
    "    '''Old code for calculating overlapping entities between two spacy documents. Also checks for matching labels if label=1.\n",
    "    \n",
    "    Return:\n",
    "        Dictionaries with the mapping of matching entity indices:\n",
    "            keys: entity index from one annotation\n",
    "            value: matched entity index from other annotation\n",
    "        \n",
    "        Ex: \"{1 : [2] , 3 : [4,5]}\" means that entity 1 from doc1 matches entity 1 in doc2, and entity 3 in doc1 matches \n",
    "        entity 4 and 5 from doc2.\n",
    "    '''\n",
    "    \n",
    "    doc1_matches = dict()\n",
    "    doc2_matches = dict()\n",
    "\n",
    "    for index1,ent1 in enumerate(doc1_ents):\n",
    "        for index2,ent2 in enumerate(doc2_ents):\n",
    "            if (ent1.end_char >= ent2.start_char) & (ent1.start_char <= ent2.end_char) & ((labels==0) | (ent1.label_ == ent2.label_)):\n",
    "                if index1 not in doc1_matches.keys():\n",
    "                    doc1_matches[index1] = [index2]\n",
    "                else:\n",
    "                    doc1_matches[index1].append(index2)\n",
    "                if index2 not in doc2_matches.keys():\n",
    "                    doc2_matches[index2] = [index1]\n",
    "                else:\n",
    "                    doc2_matches[index2].append(index1)\n",
    "                \n",
    "    return doc1_matches, doc2_matches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "00a8e6a7-7116-405c-addd-b19e08a9fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(doc1_ents, doc2_ents, labels):\n",
    "    '''calculate whether two ents have exact overlap\n",
    "    returns bool\n",
    "    '''\n",
    "    \n",
    "    doc1_matches = dict()\n",
    "    doc2_matches = dict()\n",
    "\n",
    "    doc1_ent_dict = dict()\n",
    "    doc2_ent_dict = dict()\n",
    "    \n",
    "    for index1,ent1 in enumerate(doc1_ents):\n",
    "        if labels == 1: #If checking for labels, then include this in the tuple's to-be-compared elements\n",
    "            doc1_ent_dict[(ent1.start_char,ent1.end_char,ent1.label_)] = index1\n",
    "        else:\n",
    "            doc1_ent_dict[(ent1.start_char,ent1.end_char)] = index1\n",
    "            \n",
    "    for index2,ent2 in enumerate(doc2_ents):\n",
    "        if labels == 1:    \n",
    "            doc2_ent_dict[(ent2.start_char,ent2.end_char,ent2.label_)] = index2\n",
    "        else:\n",
    "            doc2_ent_dict[(ent2.start_char,ent2.end_char)] = index2\n",
    "        \n",
    "    doc1_ent_set = set(doc1_ent_dict.keys())\n",
    "    doc2_ent_set = set(doc2_ent_dict.keys())\n",
    "    \n",
    "    matched_ents = doc1_ent_set.intersection(doc2_ent_set)\n",
    "    \n",
    "    for match in matched_ents:\n",
    "        index1 = doc1_ent_dict[match]\n",
    "        index2 = doc2_ent_dict[match]\n",
    "        doc1_matches[index1] = [index2]\n",
    "        doc2_matches[index2] = [index1]\n",
    "        \n",
    "    return doc1_matches, doc2_matches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "650bc405-f61d-4287-ac88-14ed6072bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agreement(doc1, doc2, loose=1, labels=1):\n",
    "    '''Calculates confusion matrix for agreement between two documents.\n",
    "    \n",
    "       returns true positive, false positive, and false negative\n",
    "    '''\n",
    "    tp = 0 #True Positives\n",
    "    fp = 0 #False Positives\n",
    "    fn = 0 #False Negatives\n",
    "    \n",
    "    doc1_ents = doc1.ents\n",
    "    doc2_ents = doc2.ents\n",
    "    \n",
    "    if loose:\n",
    "        doc1_matches, doc2_matches = overlaps(doc1_ents, doc2_ents, labels)\n",
    "    else:\n",
    "        doc1_matches, doc2_matches = exact_match(doc1_ents, doc2_ents, labels)\n",
    "    \n",
    "    doc1_match_num = len(doc1_matches.keys())\n",
    "    doc2_match_num = len(doc2_matches.keys())\n",
    "    \n",
    "    duplicate_matches = 0\n",
    "    for value in doc2_matches.values():\n",
    "        duplicate_matches += len(value) - 1\n",
    "    \n",
    "    tp = doc1_match_num - duplicate_matches #How many entity indices from doc1 matched, minus duplicated matches\n",
    "    fp = len(doc2_ents) - doc2_match_num #How many entities from doc2 that didn't match\n",
    "    fn = len(doc1_ents) - doc1_match_num #How many entities from doc1 that didn't match\n",
    "    \n",
    "    return (tp, fp, fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b609c982-0b94-4913-b23b-a672950bd99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_f1(tp,fp,fn):\n",
    "    '''calculate f1 given true positive, false positive, and false negative values'''\n",
    "    \n",
    "    return (2*tp)/float(2*tp+fp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0b3a52c-8d6b-4a15-a7ae-5fc992f4564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_agreement(docs1, docs2, loose=1, labels=1):\n",
    "    '''calculate f1 over an entire corpus of documents'''\n",
    "    corpus_tp, corpus_fp, corpus_fn = (0,0,0)\n",
    "    \n",
    "    for i, doc1 in enumerate(docs1):\n",
    "        tp,fp,fn = agreement(doc1, docs2[i],loose)\n",
    "        corpus_tp += tp\n",
    "        corpus_fp += fp\n",
    "        corpus_fn += fn\n",
    "    \n",
    "    return pairwise_f1(tp,fp,fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3.8]",
   "language": "python",
   "name": "conda-env-python3.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
